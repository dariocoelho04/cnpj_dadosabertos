{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configurando a Conexão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar a conexão PostgreSQL\n",
    "db_user = 'admin'\n",
    "db_password = 'PstgsqlAdm2024!$'\n",
    "db_host = 'localhost'\n",
    "db_port = '5432'\n",
    "db_name = 'empresasbr_db'\n",
    "\n",
    "# String de conexão do SQLAlchemy\n",
    "engine = create_engine(f'postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Funções Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para verificar quais registros já existem no banco de dados em lotes menores\n",
    "def check_existing_records(connection, table_name, primary_key_column, batch, chunk_size=10000):\n",
    "    existing_records = set()\n",
    "    keys = tuple(batch[primary_key_column].unique())\n",
    "\n",
    "    if not keys:\n",
    "        return existing_records\n",
    "\n",
    "    for i in range(0, len(keys), chunk_size):\n",
    "        chunk = keys[i:i + chunk_size]\n",
    "        query = text(f\"SELECT {primary_key_column} FROM {table_name} WHERE {primary_key_column} IN :keys\")\n",
    "        result = connection.execute(query, {'keys': chunk}).fetchall()\n",
    "        existing_records.update([row[0] for row in result])\n",
    "\n",
    "    return existing_records\n",
    "\n",
    "# Função para carregar os dados do Parquet e inserir no PostgreSQL com tratamento de duplicidade\n",
    "def insert_dask_dataframe_to_postgres(parquet_file, table_name, primary_key_column):\n",
    "    try:\n",
    "        # Carregar o arquivo Parquet com Dask\n",
    "        df = dd.read_parquet(parquet_file)\n",
    "\n",
    "        # Lista de colunas de data\n",
    "        colunas_data = [\n",
    "            'data_situacao_cadastral', 'data_inicio_atividade', 'data_situacao_especial',\n",
    "            'data_entrada_sociedade', 'data_opcao_simples', 'data_exclusao_simples', \n",
    "            'data_opcao_mei', 'data_exclusao_mei'\n",
    "        ]\n",
    "\n",
    "        # Substituir NaT por None nas colunas de data\n",
    "        df[colunas_data] = df[colunas_data].map_partitions(lambda part: part.astype('object').where(part.notna(), None))\n",
    "\n",
    "        # Inserção por lotes\n",
    "        batch_size = 10000\n",
    "\n",
    "        with engine.connect() as connection:\n",
    "            trans = connection.begin()\n",
    "\n",
    "            try:\n",
    "                for i in range(0, len(df), batch_size):\n",
    "                    # Computar o lote como Pandas DataFrame para inserção\n",
    "                    batch = df.loc[i:i + batch_size].compute()\n",
    "\n",
    "                    if batch.empty:\n",
    "                        print(f\"Lote {i // batch_size + 1} está vazio. Interrompendo o processo...\")\n",
    "                        break\n",
    "\n",
    "                    # Substituir valores nulos por None no Pandas DataFrame\n",
    "                    batch = batch.where(pd.notnull(batch), None)\n",
    "\n",
    "                    # Verificar registros que já existem no banco de dados\n",
    "                    existing_records = check_existing_records(connection, table_name, primary_key_column, batch)\n",
    "\n",
    "                    # Filtrar apenas os registros que não estão no banco\n",
    "                    new_records = batch[~batch[primary_key_column].isin(existing_records)]\n",
    "\n",
    "                    if new_records.empty:\n",
    "                        print(f\"Lote {i // batch_size + 1} já foi inserido previamente. Pulando...\")\n",
    "                        continue\n",
    "\n",
    "                    # Gerar a query de inserção com upsert (ON CONFLICT)\n",
    "                    insert_query = generate_upsert_query(table_name, new_records.columns, primary_key_column)\n",
    "\n",
    "                    # Executar o upsert usando `text()` para preparar a query\n",
    "                    connection.execute(text(insert_query), new_records.to_dict(orient='records'))\n",
    "                    print(f\"Lote {i // batch_size + 1} inserido com sucesso.\")\n",
    "                \n",
    "                trans.commit()\n",
    "                print(f\"Dados inseridos ou atualizados com sucesso na tabela {table_name}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                trans.rollback()\n",
    "                print(f\"Erro ao inserir lote {i // batch_size + 1}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao inserir dados na tabela {table_name}: {e}\")\n",
    "\n",
    "# Função para gerar uma query de inserção com ON CONFLICT para evitar duplicatas\n",
    "def generate_upsert_query(table_name, columns, primary_key_column):\n",
    "    insert_cols = ', '.join(columns)\n",
    "    value_placeholders = ', '.join([f':{col}' for col in columns])\n",
    "    update_clause = ', '.join([f\"{col} = EXCLUDED.{col}\" for col in columns if col != primary_key_column])\n",
    "\n",
    "    query = f\"\"\"\n",
    "    INSERT INTO {table_name} ({insert_cols})\n",
    "    VALUES ({value_placeholders})\n",
    "    ON CONFLICT ({primary_key_column})\n",
    "    DO UPDATE SET {update_clause};\n",
    "    \"\"\"\n",
    "    return query\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execução da importação para o PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lote 1 inserido com sucesso.\n",
      "Lote 2 inserido com sucesso.\n",
      "Lote 3 inserido com sucesso.\n",
      "Lote 4 inserido com sucesso.\n",
      "Lote 5 inserido com sucesso.\n",
      "Lote 6 inserido com sucesso.\n",
      "Lote 7 inserido com sucesso.\n",
      "Lote 8 inserido com sucesso.\n",
      "Lote 9 inserido com sucesso.\n",
      "Lote 10 inserido com sucesso.\n",
      "Lote 11 inserido com sucesso.\n",
      "Lote 12 inserido com sucesso.\n",
      "Lote 13 inserido com sucesso.\n",
      "Lote 14 inserido com sucesso.\n",
      "Lote 15 inserido com sucesso.\n",
      "Lote 16 inserido com sucesso.\n",
      "Lote 17 inserido com sucesso.\n",
      "Lote 18 inserido com sucesso.\n",
      "Lote 19 inserido com sucesso.\n",
      "Lote 20 inserido com sucesso.\n",
      "Lote 21 inserido com sucesso.\n",
      "Lote 22 inserido com sucesso.\n",
      "Lote 23 inserido com sucesso.\n",
      "Lote 24 inserido com sucesso.\n",
      "Lote 25 inserido com sucesso.\n",
      "Lote 26 inserido com sucesso.\n",
      "Lote 27 inserido com sucesso.\n",
      "Lote 28 inserido com sucesso.\n",
      "Lote 29 inserido com sucesso.\n",
      "Lote 30 está vazio. Interrompendo o processo...\n",
      "Dados inseridos ou atualizados com sucesso na tabela empresas.\n"
     ]
    }
   ],
   "source": [
    "# Executar a função para inserir no PostgreSQL\n",
    "parquet_file = './parquets/dataset_final.parquet'\n",
    "insert_dask_dataframe_to_postgres(parquet_file, 'empresas', 'cnpj_completo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
